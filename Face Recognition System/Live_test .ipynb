{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6079bbec-d20c-4711-bdf1-456f8f43595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from mtcnn import MTCNN\n",
    "from keras_facenet import FaceNet\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3094e599-4ad3-468c-ab93-2288ce5ff562",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = r\"C:\\Users\\hp\\project\\cropped_dataset\"\n",
    "IMG_SIZE = (160, 160)\n",
    "MARGIN = 20\n",
    "AUG_PER_IMAGE = 3          \n",
    "UNKNOWN_THRESHOLD = 0.60   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c18433-7c68-43e9-beb5-cb931e957c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_PATH = r\"C:\\Users\\hp\\project\\svm_model.pkl\"\n",
    "ENCODER_PATH = r\"C:\\Users\\hp\\project\\encoder.pkl\"\n",
    "SCALER_PATH = r\"C:\\Users\\hp\\project\\scaler.pkl\"\n",
    "RF_PATH = r\"C:\\Users\\hp\\project\\rf_model.pkl\"\n",
    "L2_FLAG_PATH = r\"C:\\Users\\hp\\project\\l2_normalizer.note\"\n",
    "FACENET_NAME = r\"C:\\Users\\hp\\project\\facenet_embedder.note\"\n",
    "YOLO_MODEL_PATH = r\"C:\\Users\\hp\\project\\yolov8n.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1586ac85-f371-48a0-86bc-4dde9c1f25b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedReader>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\lz4\\frame\\__init__.py\", line 753, in flush\n",
      "    self._fp.flush()\n",
      "ValueError: I/O operation on closed file.\n",
      "Exception ignored in: <_io.BufferedReader>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\lz4\\frame\\__init__.py\", line 753, in flush\n",
      "    self._fp.flush()\n",
      "ValueError: I/O operation on closed file.\n",
      "Exception ignored in: <_io.BufferedReader>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\lz4\\frame\\__init__.py\", line 753, in flush\n",
      "    self._fp.flush()\n",
      "ValueError: I/O operation on closed file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detector = MTCNN()\n",
    "embedder = FaceNet()\n",
    "yolo_model = YOLO(\"yolov8n.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b7ebfef-f752-49f7-b45e-c65a38f312a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_crop_with_margin(box, img_w, img_h, margin=MARGIN):\n",
    "    x, y, w, h = box\n",
    "    x, y = abs(x), abs(y)\n",
    "    x1, y1 = max(0, x - margin), max(0, y - margin)\n",
    "    x2, y2 = min(img_w, x + w + margin), min(img_h, y + h + margin)\n",
    "    bw, bh = x2 - x1, y2 - y1\n",
    "    side = max(bw, bh)\n",
    "    cx = x1 + bw // 2\n",
    "    cy = y1 + bh // 2\n",
    "    x1 = max(0, cx - side // 2)\n",
    "    y1 = max(0, cy - side // 2)\n",
    "    x2 = min(img_w, x1 + side)\n",
    "    y2 = min(img_h, y1 + side)\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27db595-7165-45f8-b5fa-1d26a7efaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_face(image, box, landmarks=None):\n",
    "    if landmarks is not None:\n",
    "        left_eye = np.array(landmarks[0][0])  \n",
    "        right_eye = np.array(landmarks[1][0])\n",
    "        angle = np.arctan2(right_eye[1] - left_eye[1], right_eye[0] - left_eye[0]) * 180 / np.pi\n",
    "        M = cv2.getRotationMatrix2D((image.shape[1]//2, image.shape[0]//2), angle, 1.0)\n",
    "        aligned = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
    "        x1, y1, x2, y2 = square_crop_with_margin(box, image.shape[1], image.shape[0])\n",
    "        return aligned[y1:y2, x1:x2]\n",
    "    else:\n",
    "        x1, y1, x2, y2 = square_crop_with_margin(box, image.shape[1], image.shape[0])\n",
    "        return image[y1:y2, x1:x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e679366-04fa-4dd4-91a0-19ceeaeea80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(path, required_size=IMG_SIZE):\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        print(f\" Could not read image: {path}\")\n",
    "        return None\n",
    "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(rgb)\n",
    "    if not results:\n",
    "        results_yolo = yolo_model(rgb)\n",
    "        boxes = results_yolo[0].boxes\n",
    "        if len(boxes) > 0:\n",
    "            for box in boxes:\n",
    "                if int(box.cls) == 0:  # Class 0: 'person'\n",
    "                    box_coords = box.xyxy[0].cpu().numpy()\n",
    "                    x1, y1, x2, y2 = map(int, box_coords)\n",
    "                    face = rgb[y1:y2, x1:x2]\n",
    "                    h, w = face.shape[:2]\n",
    "                    center_x, center_y = w // 2, h // 2\n",
    "                    angle = 0  \n",
    "                    M = cv2.getRotationMatrix2D((center_x, center_y), angle, 1.0)\n",
    "                    face = cv2.warpAffine(face, M, (w, h))\n",
    "                    break\n",
    "            else:\n",
    "                print(f\" No 'person' detected in fallback: {path}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\" No detection in fallback: {path}\")\n",
    "            return None\n",
    "    else:\n",
    "        box = results[0]['box']\n",
    "        landmarks = [[results[0]['keypoints']['left_eye']], [results[0]['keypoints']['right_eye']]]\n",
    "        face = align_face(rgb, box, landmarks)\n",
    "    if face.size == 0:\n",
    "        return None\n",
    "    face = cv2.resize(face, required_size)\n",
    "    face = face.astype(\"float32\") / 255.0\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13befc7f-433f-4f0c-9569-29af315f5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_face(face_rgb_01):\n",
    "    aug_samples = []\n",
    "\n",
    "    flipped = cv2.flip(face_rgb_01, 1)\n",
    "    aug_samples.append(flipped)\n",
    "    \n",
    "    beta = np.random.uniform(-30, 30)\n",
    "    bgr = cv2.cvtColor((face_rgb_01*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    bright = cv2.convertScaleAbs(bgr, alpha=1.0, beta=beta)\n",
    "    bright = cv2.cvtColor(bright, cv2.COLOR_BGR2RGB).astype(np.float32)/255.0\n",
    "    aug_samples.append(bright)\n",
    "\n",
    "    angle = np.random.uniform(-15, 15)\n",
    "    M = cv2.getRotationMatrix2D((IMG_SIZE[0]//2, IMG_SIZE[1]//2), angle, 1.0)\n",
    "    rot = cv2.warpAffine((face_rgb_01*255).astype(np.uint8), M, IMG_SIZE)\n",
    "    rot = rot.astype(np.float32)/255.0\n",
    "    aug_samples.append(rot)\n",
    "\n",
    "    blur = cv2.GaussianBlur((face_rgb_01*255).astype(np.uint8), (3,3), 0)\n",
    "    blur = blur.astype(np.float32)/255.0\n",
    "    aug_samples.append(blur)\n",
    "\n",
    "    shear = np.random.uniform(-0.1, 0.1)\n",
    "    M = np.array([[1, shear, 0], [0, 1, 0]], dtype=np.float32)\n",
    "    shear_img = cv2.warpAffine((face_rgb_01*255).astype(np.uint8), M, IMG_SIZE)\n",
    "    shear_img = shear_img.astype(np.float32)/255.0\n",
    "    aug_samples.append(shear_img)\n",
    "    np.random.shuffle(aug_samples)\n",
    "    return aug_samples[:AUG_PER_IMAGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7639e4d6-08dc-49fa-876f-e22993692514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_to_embedding(face_rgb_01):\n",
    "    face_255 = (face_rgb_01*255).astype(np.uint8)\n",
    "    face_255 = np.expand_dims(face_255, 0)\n",
    "    embs = embedder.embeddings(face_255)\n",
    "    return embs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18a2fb10-011c-4fbf-9209-fd1fca2f102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(svm_path=SVM_PATH, rf_path=RF_PATH, encoder_path=ENCODER_PATH, scaler_path=SCALER_PATH):\n",
    "    if not (os.path.exists(svm_path) and os.path.exists(rf_path) and os.path.exists(encoder_path) and os.path.exists(scaler_path)):\n",
    "        print(f\" Model files missing: {svm_path}, {rf_path}, {encoder_path}, {scaler_path}\")\n",
    "        return None, None, None, None, None\n",
    "    try:\n",
    "        svm = joblib.load(svm_path)\n",
    "        rf = joblib.load(rf_path)\n",
    "        encoder = joblib.load(encoder_path)\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        l2 = Normalizer(norm=\"l2\")\n",
    "        print(f\"Loaded SVM, RF, encoder, scaler, and L2 normalizer\")\n",
    "        return svm, rf, encoder, scaler, l2\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {str(e)}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0624614-9d5f-4321-b243-538a4dccc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realtime_recognition():\n",
    "    svm, rf, encoder, scaler, l2 = load_models()\n",
    "    if svm is None:\n",
    "        return\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "    win_name = \"Real-Time Face Recognition (Press 'q' to quit)\"\n",
    "    pred_queues = {}  \n",
    "    MIN_QUEUE_SIZE = 2  \n",
    "\n",
    "    def match_box(new_box, old_boxes):\n",
    "        if not old_boxes:\n",
    "            return None\n",
    "        nx1, ny1, nw, nh = new_box\n",
    "        new_center = (nx1 + nw / 2, ny1 + nh / 2)\n",
    "        min_dist = float('inf')\n",
    "        matched_key = None\n",
    "        for old_box in old_boxes:\n",
    "            ox1, oy1, ow, oh = old_box\n",
    "            old_center = (ox1 + ow / 2, oy1 + oh / 2)\n",
    "            dist = ((new_center[0] - old_center[0])**2 + (new_center[1] - old_center[1])**2)**0.5\n",
    "            if dist < min_dist and dist < 100:  \n",
    "                min_dist = dist\n",
    "                matched_key = tuple(old_box)\n",
    "        return matched_key\n",
    "\n",
    "    last_boxes = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\" Failed to grab frame\")\n",
    "                break\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            faces = []\n",
    "            boxes_to_display = []\n",
    "            results = detector.detect_faces(rgb)\n",
    "            if results:\n",
    "                for res in results:\n",
    "                    box = res['box']\n",
    "                    landmarks = [[res['keypoints']['left_eye']], [res['keypoints']['right_eye']]]\n",
    "                    face = align_face(rgb, box, landmarks)\n",
    "                    if face.size == 0:\n",
    "                        continue\n",
    "                    faces.append((face, box))\n",
    "                    boxes_to_display.append(box)\n",
    "            else:\n",
    "                results_yolo = yolo_model(rgb)\n",
    "                boxes = results_yolo[0].boxes\n",
    "                for box in boxes:\n",
    "                    if int(box.cls) != 0:  \n",
    "                        continue\n",
    "                    box_coords = box.xyxy[0].cpu().numpy()\n",
    "                    x1, y1, x2, y2 = map(int, box_coords)\n",
    "                    face = align_face(rgb, [x1, y1, x2-x1, y2-y1])\n",
    "                    if face.size == 0:\n",
    "                        continue\n",
    "                    faces.append((face, [x1, y1, x2-x1, y2-y1]))\n",
    "                    boxes_to_display.append([x1, y1, x2-x1, y2-y1])\n",
    "            last_boxes = boxes_to_display\n",
    "            if not faces:\n",
    "                cv2.imshow(win_name, frame)\n",
    "                continue\n",
    "   \n",
    "            for face, box in faces:\n",
    "                face = cv2.resize(face, IMG_SIZE).astype(np.float32) / 255.0\n",
    "                emb = face_to_embedding(face).reshape(1, -1)\n",
    "                emb = l2.transform(emb)\n",
    "                emb = scaler.transform(emb)\n",
    "                probs_svm = svm.predict_proba(emb)[0]\n",
    "                probs_rf = rf.predict_proba(emb)[0]\n",
    "                probs = (probs_svm + probs_rf) / 2\n",
    "                pred_idx = np.argmax(probs)\n",
    "                pred_conf = probs[pred_idx]\n",
    "                box_key = match_box(box, last_boxes)\n",
    "                \n",
    "                if box_key is None:\n",
    "                    box_key = tuple(box)\n",
    "                if box_key not in pred_queues:\n",
    "                    pred_queues[box_key] = deque(maxlen=5)\n",
    "                pred_queues[box_key].append((pred_idx, pred_conf))\n",
    "                if len(pred_queues[box_key]) >= MIN_QUEUE_SIZE:\n",
    "                    avg_conf = np.mean([conf for _, conf in pred_queues[box_key]])\n",
    "                    mode_idx = max(set([idx for idx, _ in pred_queues[box_key]]), key=[idx for idx, _ in pred_queues[box_key]].count)\n",
    "                    pred_name = encoder.inverse_transform([mode_idx])[0]\n",
    "                    display = f\"{pred_name} ({avg_conf*100:.1f}%)\" if avg_conf >= UNKNOWN_THRESHOLD else f\"Unknown ({avg_conf*100:.1f}%)\"\n",
    "                    color = (0, 255, 0) if avg_conf >= UNKNOWN_THRESHOLD else (0, 0, 255)\n",
    "                else:\n",
    "                    pred_name = encoder.inverse_transform([pred_idx])[0]\n",
    "                    display = f\"{pred_name} ({pred_conf*100:.1f}%)\" if pred_conf >= UNKNOWN_THRESHOLD else f\"Unknown ({pred_conf*100:.1f}%)\"\n",
    "                    color = (0, 255, 0) if pred_conf >= UNKNOWN_THRESHOLD else (0, 0, 255)\n",
    "                x1, y1, x2, y2 = square_crop_with_margin(box, rgb.shape[1], rgb.shape[0])\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, display, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "                \n",
    "            active_keys = [tuple(box) for box in boxes_to_display]\n",
    "            pred_queues = {k: q for k, q in pred_queues.items() if k in active_keys}\n",
    "            cv2.imshow(win_name, frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                print(\"ðŸ‘‹ Closing camera...\")\n",
    "                break\n",
    "            if cv2.getWindowProperty(win_name, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51e7e1ac-8d05-453d-9e18-7b13c6b4de7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SVM, RF, encoder, scaler, and L2 normalizer\n",
      "Loaded SVM, RF, encoder, scaler, and L2 normalizer\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "ðŸ‘‹ Closing camera...\n"
     ]
    }
   ],
   "source": [
    "svm, rf, encoder, scaler, l2 = load_models()\n",
    "realtime_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0258fce2-f9d4-4fbd-a088-5b4fb3a86112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
